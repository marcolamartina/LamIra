# -*- coding: utf-8 -*-
"""Object_classification_Random_Forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qHtQTHvvrXpnpanI__QRRnK1J3QN2bvX

# Object classification
"""
from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score
from sklearn.linear_model import SGDClassifier 
from sklearn.model_selection import train_test_split
import numpy as np
import os
import ast
from glob import glob
import random
import traceback
from tabulate import tabulate
import pickle

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from matplotlib import pyplot as plt
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

"""## Parameters"""

new_data=True
load_old_params=False
save_params=False
selected_space=True
kron=False
print_matrix=True
space="texture"
classifier="Random Forest"
dictionary_index={"shape":0,"color":1,"texture":2}


"""## Utils functions"""

def translate(name):
    translate_dict={"apple":"mela",
                    "ball":"palla",
                    "bell pepper":"peperone",
                    "binder":"raccoglitore",
                    "bowl":"ciotola",
                    "calculator":"calcolatrice",
                    "camera":"fotocamera",
                    "cell phone":"telefono",
                    "cereal box":"scatola",
                    "coffee mug":"tazza",
                    "comb":"spazzola",
                    "dry battery":"batteria",
                    "flashlight":"torcia",
                    "food box":"scatola",
                    "food can":"lattina",
                    "food cup":"barattolo",
                    "food jar":"barattolo",
                    "garlic":"aglio",
                    "lemon":"limone",
                    "lime":"lime",
                    "onion":"cipolla",
                    "orange":"arancia",
                    "peach":"pesca",
                    "pear":"pera",
                    "potato":"patata",
                    "tomato":"pomodoro",
                    "soda can":"lattina",
                    "marker":"pennarello",
                    "plate":"piatto",
                    "notebook":"quaderno",
                    "keyboard":"tastiera",
                    "glue stick":"colla",
                    "sponge":"spugna",
                    "toothpaste":"dentifricio",
                    "toothbrush":"spazzolino"
                    }
    try:
        return translate_dict[name]
    except:
        return name

def normalize_color(color):
    return color
    color_normalized=[]
    for i,f in enumerate(color):
        if i%3==0:
            color_normalized.append(f/256)
        else:
            color_normalized.append((f+128)/256)
    return color_normalized


def sort_and_cut_dict(dictionary,limit=3):
    iterator=sorted(dictionary.items(), key=lambda item: item[1], reverse=True)[:limit]
    coef=sum([i[1] for i in iterator])
    return {k: v/coef for k, v in iterator}

"""## Data"""

obj_dir = "/content/drive/My Drive/Tesi/Code/Object_classification"
obj_dir = "/Users/marco/Google Drive/Tesi/Code/Object_classification"
data_dir = obj_dir+"/Data"
model_filename = obj_dir+"/model.pkl"
exclusion_list=["binder","camera","cell phone","dry battery"]
test_folder=["apple_3",
             "bell_pepper_1",
             "bowl_3",
             "cereal_box_1",
             "coffe_mug_5",
             "comb_5",
             "flashlight_4",
             "food_box_6",
             "food_can_2",
             "garlic_1",
             "glue_stick_3",
             "keyboard_2",
             "lemon_1",
             "lime_1",
             "onion_1",
             "orange_1",
             "pear_4",
             "plate_5",
             "potato_5",
             "soda_can_2",
             "sponge_8",
             "tomato_1",
             "toothbrush_2"
             ]
if new_data:
    color_train=[]
    shape_train=[]
    texture_train=[]
    color_test=[]
    shape_test=[]
    texture_test=[]
    y_train=[]
    y_test=[]
    file_list=glob(data_dir+'/**', recursive=True)
    number_of_files=len(file_list)
    with open(obj_dir+"/dictionary.pickle","rb") as f:
        dictionary=pickle.load(f)
        for j,filename in enumerate(file_list):
            if os.path.isfile(filename) and filename.endswith(".txt"):
                print("{:.2f}%".format(j*100/number_of_files),end="\r")
                name=" ".join(filename.split("_")[:-3]).rsplit("/", 1)[1]
                if name in exclusion_list:
                    continue
                name=translate(name)
                folder=filename.split("/")[-2]

                if folder not in dictionary.keys():
                    continue
                with open(filename, "r") as f:
                    features=[]
                    try:
                        lines=f.readlines()
                        for line in lines:
                            features.append(ast.literal_eval(line))
                        if len(features)==3:        
                            color,shape,texture=features
                            color=normalize_color(color)
                            if folder in test_folder:
                                color_test.append(color)
                                shape_test.append(shape)
                                texture_test.append(texture)
                                if selected_space:
                                    y_test.append(folder)
                                else:    
                                    y_test.append(name)
                            else:
                                color_train.append(color)
                                shape_train.append(shape)
                                texture_train.append(texture)    
                                if selected_space:
                                    y_train.append(folder)
                                else:    
                                    y_train.append(name)
                    except:
                        print("Error in {}".format(filename))
                        continue    
        y_train=np.array(y_train)
        y_test=np.array(y_test)
        color_train=np.array(color_train)
        shape_train=np.array(shape_train)
        texture_train=np.array(texture_train)
        color_test=np.array(color_test)
        shape_test=np.array(shape_test)
        texture_test=np.array(texture_test)
        if not selected_space:
            if kron:
                X_train=np.array([np.kron(np.kron(c,s),t) for c,s,t in zip(color_train,shape_train,texture_train)])
                X_test=np.array([np.kron(np.kron(c,s),t) for c,s,t in zip(color_test,shape_test,texture_test)])
            else:
                X_train=np.array([np.concatenate((c, s, t), axis=None) for c,s,t in zip(color_train,shape_train,texture_train)])
                X_test=np.array([np.concatenate((c, s, t), axis=None) for c,s,t in zip(color_test,shape_test,texture_test)])
        
        else:
            if space=="shape":
                X_train=shape_train
                X_test=shape_test
            elif space=="color":
                X_train=color_train
                X_test=color_test    
            elif space=="texture":
                X_train=texture_train
                X_test=texture_test
        
        
            
    
else:
    X_train=np.load(obj_dir+"/input_train.npy")
    X_test=np.load(obj_dir+"/input_test.npy")
    color_train=np.load(obj_dir+"/color_train.npy")
    shape_train=np.load(obj_dir+"/shape_train.npy")
    texture_train=np.load(obj_dir+"/texture_train.npy")
    color_test=np.load(obj_dir+"/color_test.npy")
    shape_test=np.load(obj_dir+"/shape_test.npy")
    texture_test=np.load(obj_dir+"/texture_test.npy")
    y_train=np.load(obj_dir+"/output_train.npy") 
    y_test=np.load(obj_dir+"/output_test.npy")

"""## Save input data"""

if selected_space:
    new_y_train=[]    
    for i in y_train:
        new_label=dictionary[i][dictionary_index[space]]
        #new_label=new_label.split("-")[0]
        new_y_train.append(new_label)
    new_y_test=[]    
    for i in y_test:
        new_label=dictionary[i][dictionary_index[space]]
        #new_label=new_label.split("-")[0]
        new_y_test.append(new_label)
    y_train=np.array(new_y_train)
    y_test=np.array(new_y_test)

if new_data and save_params:
    np.save(obj_dir+"/input_train.npy",X_train)
    np.save(obj_dir+"/input_test.npy",X_test)
    np.save(obj_dir+"/color_train.npy",color_train)
    np.save(obj_dir+"/shape_train.npy",shape_train)
    np.save(obj_dir+"/texture_train.npy",texture_train)
    np.save(obj_dir+"/color_test.npy",color_test)
    np.save(obj_dir+"/shape_test.npy",shape_test)
    np.save(obj_dir+"/texture_test.npy",texture_test)
    np.save(obj_dir+"/output_test.npy",y_test)
    np.save(obj_dir+"/output_train.npy",y_train)

"""## Classifier fitting"""

if load_old_params:
    with open(model_filename, 'rb') as file:
        clf = pickle.load(file)
else:
    if classifier=="Random Forest":
        clf = RandomForestClassifier(n_jobs=-1, n_estimators=30)
    elif classifier=="SVC":     
        clf=SVC()
    elif classifier=="SGDClassifier":    
        clf = make_pipeline(StandardScaler(), SGDClassifier())
    else:
        print("Bad classifier")
        exit(0)     

    clf.fit(X_train,y_train)
    #print(clf.score(X_test,y_test))

"""## Saving parameters"""

if save_params:
    with open(model_filename, 'wb') as file:
        pickle.dump(clf, file)

"""## Score"""

def classify_prediction(prediction):
    sure=[]
    unsure=[]
    dubious=[]
    cannot_answer=[]
    for pred in prediction:
        o,p=pred
        values=list(p.values())
        keys=list(p.keys())
        # sure
        if values[0]>0.8: 
            sure.append(pred)
        # unsure        
        elif values[0]>0.6:
            unsure.append(pred)
        # dubious    
        elif values[0]>0.4:
            dubious.append(pred)
        # cannot_answer
        else:
            cannot_answer.append(pred)
    return {"sure":sure, "unsure":unsure, "dubious":dubious, "cannot_answer":cannot_answer}               

def calculate_accuracy(category,prediction):
    counter=0
    if category=="dubious":
        for o,p in pred:
            if o in list(p.keys())[0:2]:
                counter+=1
    elif category=="cannot_answer":
        for o,p in pred:
            if o not in list(p.keys())[0:2]:
                counter+=1
    else:
        for o,p in pred:
            if o.split("-")[0] in list(p.keys())[0]:
                counter+=1                       
    return counter/len(pred)
if False:
    label_prob=clf.predict_proba(X_test)
    pred=[[y_test[j],sort_and_cut_dict({clf.classes_[i]:v for i,v in enumerate(row)})] for j,row in enumerate(label_prob)]
    pred_classified=classify_prediction(pred)
    print("TOTAL TEST: {}".format(len(pred)))
    for l,pred in pred_classified.items():
        print(l.upper())
        print(40*"-")
        selected=[]
        for o,p in pred:
            if l=="dubious" and o not in list(p.keys())[0:2]:
                selected.append([o,", ".join([str(a)+":"+str(round(b,2)) for a,b in list(p.items())])])
            elif l=="cannot_answer" and o in list(p.keys())[0:2]:
                selected.append([o,", ".join([str(a)+":"+str(round(b,2)) for a,b in list(p.items())])])

            elif l=="unsure" and o.split("-")[0] not in list(p.keys())[0]:
                selected.append([o,", ".join([str(a)+":"+str(round(b,2)) for a,b in list(p.items())])])

            elif (l=="sure") and o != list(p.keys())[0]:
                selected.append([o,", ".join([str(a)+":"+str(round(b,2)) for a,b in list(p.items())])])
        print(tabulate(selected, headers=['Original','Predicted']))
        #print("Not correct: {}/{} - {:.2f}%".format(len(selected),len(pred),len(selected)*100/len(pred)))
        #accuracy=calculate_accuracy(l,pred)
        #print("Accuracy: {:.2f}".format(accuracy))

"""## Test"""

#clf.score(X_test,y_test)

#plt.plot(clf.feature_importances_)

#clf.feature_importances_

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import pandas as pd
def classification_report(y_true, y_pred):
    accuracy=accuracy_score(y_true, y_pred)
    precision=precision_score(y_true, y_pred, average='weighted')#, zero_division=True)
    recall=recall_score(y_true, y_pred, average='weighted')
    f1=f1_score(y_true, y_pred, average='weighted')
    '''
    print(f"Accuracy: {accuracy}.")
    print(f"Precision: {precision}.")
    print(f"Recall: {recall}.")
    print(f"F1-Score: {f1}.")

    print("\nSuddivisione per Classe")
    '''
    matrix = confusion_matrix(y_true, y_pred)

    # i falsi positivi si trovano sommando le colonne ed eliminando l'elemento diagonale (che rappresenta i veri positivi)
    FP = matrix.sum(axis=0) - np.diag(matrix)  
    # i falsi negativi invece si individuano sommando le righe
    FN = matrix.sum(axis=1) - np.diag(matrix)
    TP = np.diag(matrix)
    TN = matrix.sum() - (FP + FN + TP)
    '''
    print("FP",FP)
    print("FN",FN)
    print("TP",TP)
    print("TN",TN)
    '''
    class_names = np.unique(np.append(y_true,[y_pred]))

    metrics_per_class = {}
    class_accuracies = (TP+TN)/(TP+TN+FP+FN)
    class_precisions = TP/(TP+FP)
    class_recalls = TP/(TP+FN)
    class_f1_scores = (2 * class_precisions * class_recalls) / (class_precisions + class_recalls)
    i=0
    unique, counts = np.unique(y_true, return_counts=True)
    bk=dict(zip(unique, counts))
    for name in class_names:
        l = [class_accuracies.tolist().pop(i), class_precisions.tolist().pop(i), class_recalls.tolist().pop(i), class_f1_scores.tolist().pop(i), FP.tolist().pop(i), FN.tolist().pop(i)]
        for j,v in enumerate(l):
            if np.isnan(v):
                l[j]=0

        if name in unique and not np.isnan(l).any():
            metrics_per_class[name]=l
        i += 1
    print(bk)
    
    accuracy_total=0
    precision_total=0
    recall_total=0
    f1_total=0
    result = pd.DataFrame(metrics_per_class, index=["Accuracy", "Precision", "Recall", "F1 Score", "FP", "FN"]).transpose() 
    print()
    print(classifier,space)
    print("-"*50)
    for k,v in metrics_per_class.items():
        print("{} & {:.4f} & {:.4f} & {:.4f} & {:.4f} \\\\".format(k,*v[:4]))
        accuracy_total+=v[0]*bk[k]
        precision_total+=v[1]*bk[k]
        recall_total+=v[2]*bk[k]
        f1_total+=v[3]*bk[k]
    print("\\hline\n\\textbf{{Total}} & \\textbf{{{:.4f}}} & \\textbf{{{:.4f}}} & \\textbf{{{:.4f}}} & \\textbf{{{:.4f}}}\\\\".format(accuracy_total/len(y_true),precision_total/len(y_true),recall_total/len(y_true),f1_total/len(y_true)))
    print("\\hline\n\n")
    #print(result, end="\n\n")
    return metrics_per_class

#from sklearn.metrics import classification_report
y_true=y_test
y_pred=clf.predict(X_test)
d=classification_report(y_true, y_pred)

from sklearn.metrics import classification_report
#print(classification_report(y_true, y_pred))

if not print_matrix:
    exit(0)
filename="confusion_"+space
data=[]
labels = []
exclusion_list=[]
for k,v in d.items():
    if not np.isnan(v).any() and sum(v[:4])>0.3:
        data.append([k]+v[:4])
        labels.append(k)
    else:
        exclusion_list.append(k)    
data=np.array(data)
colors = ['red','yellow','blue','green']
df = pd.DataFrame(data.T, index=["Label","Accuracy", "Precision", "Recall", "F1 Score"]).transpose()
#df=df.set_index("Label")
df[["Accuracy", "Precision", "Recall", "F1 Score"]]=df[["Accuracy", "Precision", "Recall", "F1 Score"]].apply(pd.to_numeric) 
ax = df.plot(x="Label", y=["Accuracy", "Precision", "Recall", "F1 Score"], kind="barh",figsize=(10,8),xlim=(0,1.0))
a=[]
b=[]
l=np.unique(y_test)
for i,e in enumerate(y_test):
    if not e in exclusion_list and clf.predict(np.array([X_test[i]]))[0] in l:
        a.append(X_test[i])
        b.append(e)   

fig=plot_confusion_matrix(clf, X_test, y_test, xticks_rotation="vertical",colorbar=False)
fig.figure_.set_size_inches(10,9)

ax=fig.ax_
ax.yaxis.label.set_size(20)
ax.xaxis.label.set_size(20)
plt.rcParams['font.size'] = '30'
for label in (ax.get_xticklabels() + ax.get_yticklabels()):
	label.set_fontsize(16)

plt.savefig("/Users/marco/Desktop/{}.pdf".format(filename), bbox_inches="tight")
#plt.show()
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Intent_classification_training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNRkJ+gPPaZOpx+p17BVonO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWnqGhiBBQVU","executionInfo":{"status":"ok","timestamp":1616625423735,"user_tz":-60,"elapsed":16188,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"d96c5287-e571-4cf2-9105-c94258178ed2"},"source":["!pip install transformers==3\n","!pip install tensorflow-gpu\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import json\n","import os\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import BertModel, BertTokenizer\n","import warnings\n","warnings.filterwarnings('ignore')\n","# specify GPU\n","device = torch.device(\"cuda\")\n","print(device)\n","# Chose Language\n","language=\"it-IT\"\n","berts={\"it-IT\":\"bert-base-multilingual-cased\", \"en-US\":\"bert-base-uncased\"}\n","class_type=\"grounding\"\n","classifications={\"grounding\":\"Grounding_Dataset\", \"learning\":\"Learning_Dataset\"}"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers==3 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n","Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.8.0rc4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.43)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.95)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n","Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.4.1)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.4.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.32.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.10.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.12.4)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.4.1)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.3.3)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow-gpu) (54.1.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.27.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.24.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.7.2)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.1)\n","cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_8us2LgBTjy","executionInfo":{"status":"ok","timestamp":1616624776588,"user_tz":-60,"elapsed":5023,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"4fc7c3ea-3c2e-4fbc-dca9-13841066a4e9"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"execution_count":268,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O5Cdjgo-G-e7","executionInfo":{"status":"ok","timestamp":1616624776591,"user_tz":-60,"elapsed":5016,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["data_dir = \"/content/drive/My Drive/Tesi/Code/Intent_classification\""],"execution_count":269,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-SWfvwOBdUC","executionInfo":{"status":"ok","timestamp":1616624776591,"user_tz":-60,"elapsed":5010,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["class LoadingData():\n","            \n","    def __init__(self):\n","        #Davide=\"Ingegneria/Magistrale/Tesi/\"\n","        train_file_path = os.path.join(data_dir,classifications[class_type],\"Train\")\n","        validation_file_path = os.path.join(data_dir,classifications[class_type],\"Validate\")\n","        category_id = 0\n","        self.cat_to_intent = {}\n","        self.intent_to_cat = {}\n","        print(train_file_path)\n","        for dirname, _, filenames in os.walk(train_file_path):\n","            for filename in filenames:\n","                \n","                file_path = os.path.join(dirname, filename)\n","                intent_id = filename.replace(\".txt\",\"\")\n","                self.cat_to_intent[category_id] = intent_id\n","                self.intent_to_cat[intent_id] = category_id\n","                category_id+=1\n","        '''Training data'''\n","        self.train_data_frame = self.file_to_dataframe(train_file_path)  \n","        self.train_data_frame = self.train_data_frame.sample(frac = 1)\n","        \n","        '''Validation data''' \n","        self.validation_data_frame = self.file_to_dataframe(validation_file_path)   \n","        self.validation_data_frame = self.validation_data_frame.sample(frac = 1)\n","\n","    def file_to_dataframe(self,file_path,file_path_2=None):\n","          data = list() \n","          for dirname, _, filenames in os.walk(file_path):\n","              for filename in filenames:\n","                  file_path = os.path.join(dirname, filename)\n","                  intent_id = filename.replace(\".txt\",\"\")\n","                  data+=self.make_data_for_intent_from_txt(file_path,intent_id,self.intent_to_cat[intent_id])\n","          if file_path_2!=None:\n","              for dirname, _, filenames in os.walk(file_path_2):\n","                  for filename in filenames:\n","                      file_path_2 = os.path.join(dirname, filename)\n","                      intent_id = filename.replace(\".txt\",\"\")\n","                      data+=self.make_data_for_intent_from_txt(file_path_2,intent_id,self.intent_to_cat[intent_id])      \n","          return pd.DataFrame(data, columns =['query', 'intent', 'category'])  \n","        \n","        \n","    def make_data_for_intent_from_txt(self,txt_file,intent_id,cat):\n","        query_list=[]\n","        with open(txt_file, 'r') as f:\n","            for line in f.readlines():\n","                text=line.replace(\"\\n\", \"\")\n","                query_list.append((text,intent_id,cat))\n","        return query_list "],"execution_count":270,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIxs5iQvBd_X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616624776592,"user_tz":-60,"elapsed":5005,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"5e5c9fed-0393-46ae-c18b-2bb52bba6489"},"source":["ld = LoadingData()"],"execution_count":271,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Tesi/Code/Intent_classification/Grounding_Dataset/Train\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lxTFA_JbBgJH","executionInfo":{"status":"ok","timestamp":1616624776592,"user_tz":-60,"elapsed":4994,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["train_df = ld.train_data_frame"],"execution_count":272,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rk9a8KyxBhcv","executionInfo":{"status":"ok","timestamp":1616624776593,"user_tz":-60,"elapsed":4990,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["label_map,id2label = ld.intent_to_cat,ld.cat_to_intent"],"execution_count":273,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cL4XkA_BhZU","executionInfo":{"status":"ok","timestamp":1616624776593,"user_tz":-60,"elapsed":4985,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["train_text, val_text, train_labels, val_labels = train_test_split(train_df['query'], train_df['category'], \n","                                                                    random_state=2018, \n","                                                                    test_size=0.2, \n","                                                                    stratify=train_df['category'])"],"execution_count":274,"outputs":[]},{"cell_type":"code","metadata":{"id":"g78suUciBhW4","executionInfo":{"status":"ok","timestamp":1616624780953,"user_tz":-60,"elapsed":9338,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["# Load the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained(berts[language])\n","bert = BertModel.from_pretrained(berts[language])"],"execution_count":275,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"mzEYB73oBhUf","executionInfo":{"status":"error","timestamp":1616625557624,"user_tz":-60,"elapsed":393,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"41d1fa6f-72f4-4d36-8101-f839a40223e3"},"source":["seq_len = [len(i.split()) for i in train_text]\n","\n","pd.Series(seq_len).hist(bins = 30)\n","max_seq_len = max(seq_len)\n","print(max_seq_len)"],"execution_count":10,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-5edb032c9ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_text' is not defined"]}]},{"cell_type":"code","metadata":{"id":"uTyXVES6BhJk","executionInfo":{"status":"ok","timestamp":1616624781202,"user_tz":-60,"elapsed":9570,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["# tokenize and encode sequences in the training set\n","if max_seq_len>512:\n","    max_seq_len = 512\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n"],"execution_count":277,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7R39qEBBhE1","executionInfo":{"status":"ok","timestamp":1616624781203,"user_tz":-60,"elapsed":9564,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"00ca6751-37a7-478b-e294-c601da7cc825"},"source":["# for train set\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","print(\"train_y:\",train_y)\n","# for validation set\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","print(\"val_y:\",val_y)\n"],"execution_count":278,"outputs":[{"output_type":"stream","text":["train_y: tensor([1, 1, 5, 0, 2, 1, 0, 3, 2, 0, 3, 1, 3, 5, 1, 0, 3, 2, 3, 3, 0, 0, 3, 2,\n","        2, 3, 1, 2, 2, 2, 3, 1, 3, 2, 2, 3, 0, 2, 1, 2, 0, 0, 2, 3, 1, 0, 1, 0,\n","        3, 0, 3, 2, 1, 1, 1, 0, 3, 3, 0, 5, 0, 3, 0, 2, 2, 2, 3, 0, 3, 3, 2, 1,\n","        4, 3, 2, 3, 2, 2, 2, 0, 3, 3, 1, 1, 3, 1, 2, 1, 0, 5, 1, 1, 0, 0, 2, 1,\n","        0, 2, 3, 3, 0, 1, 0, 3, 2, 3, 3, 1, 1, 0, 3, 1, 2, 0, 3, 0, 2, 0, 1, 1,\n","        0, 2, 0, 1, 1, 2, 0, 3, 1, 0, 3, 0, 0, 2, 0, 1, 1, 2, 3, 0, 1, 0, 3, 0,\n","        1, 2, 3, 4, 0, 0, 0, 3, 1, 3, 2, 3, 3, 3, 1, 0, 2, 1, 3, 4, 4, 0, 3, 4,\n","        2, 1, 3, 1, 0, 2, 2, 0, 0, 2, 2, 0, 0, 1, 1, 1, 4, 0, 1, 2, 1, 0, 5, 3,\n","        0, 2, 1, 3, 5, 2, 0, 1, 0, 0, 0, 1, 2, 2, 2, 2, 3, 5, 3, 2, 3, 1, 2, 1,\n","        2, 2, 1, 2, 2, 0, 1, 0, 3, 0, 5, 3, 3, 2, 5, 0, 0, 3, 2, 3, 0, 2, 1, 3,\n","        0, 0, 1, 2, 0, 0, 2, 0, 3, 2, 0, 1, 0, 1, 0, 0, 4, 5, 3, 4, 0, 1, 1, 2,\n","        0, 1, 4, 3, 0, 2, 3, 2, 0, 2, 3, 1, 2, 1, 4, 3, 1, 3, 1, 1, 3, 0, 1, 1,\n","        3, 0, 2, 2, 1, 0, 2, 0, 0, 0, 3, 0, 1, 3, 0, 5, 1, 0, 0, 0, 0, 2, 2, 0,\n","        0, 2, 1, 0, 1, 1, 3, 2, 3, 3, 3, 2, 2, 3, 1, 1, 3, 1, 1, 2, 3, 5, 3, 3,\n","        3, 3, 0, 1, 4, 3, 3, 5, 3, 0, 1, 3, 1, 3, 3, 0, 1, 3, 0, 1, 5, 1, 1, 2,\n","        0, 3, 0, 0, 3, 0, 0, 1, 2, 0, 1, 3, 0, 2, 2, 0, 3, 2, 2, 2, 0, 3, 2, 3,\n","        3, 0, 3, 1, 0, 1, 0, 3, 0, 1, 0, 0, 3, 3, 1, 0, 0, 1, 4, 0, 2, 2, 2, 2,\n","        2, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 2, 0, 1, 3, 2, 2, 1, 4, 3, 2, 0, 1, 3,\n","        1, 2, 0, 0, 4, 3, 3, 3, 3, 3, 1, 2])\n","val_y: tensor([0, 2, 1, 3, 2, 0, 0, 3, 3, 4, 1, 0, 1, 1, 3, 1, 1, 2, 3, 0, 3, 0, 3, 1,\n","        0, 2, 3, 0, 3, 1, 0, 2, 3, 2, 3, 0, 0, 3, 2, 1, 1, 0, 0, 1, 1, 0, 0, 2,\n","        0, 1, 3, 4, 3, 1, 0, 2, 2, 1, 4, 3, 0, 0, 5, 1, 3, 0, 2, 2, 3, 2, 1, 5,\n","        1, 2, 3, 3, 2, 1, 3, 2, 0, 2, 3, 0, 2, 3, 2, 1, 1, 0, 0, 3, 1, 3, 5, 0,\n","        0, 3, 2, 0, 1, 0, 0, 5, 2, 1, 2, 4, 0, 3, 2, 2])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PLkncx1iBnp6","executionInfo":{"status":"ok","timestamp":1616624781203,"user_tz":-60,"elapsed":9553,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 16\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"],"execution_count":279,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpyZQOzzBnn3","executionInfo":{"status":"ok","timestamp":1616624781204,"user_tz":-60,"elapsed":9549,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False"],"execution_count":280,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWmRKqgvBnlj","executionInfo":{"status":"ok","timestamp":1616625521734,"user_tz":-60,"elapsed":521,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["class BERT_Arch(nn.Module):\n","    def __init__(self, bert,label_map):\n","        super(BERT_Arch, self).__init__()\n","        self.bert = bert \n","      \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # relu activation function\n","        self.relu =  nn.ReLU()\n","\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768,512)\n","\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512,len(label_map))\n","\n","        #softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","        #pass the inputs to the model  \n","        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","\n","        x = self.fc1(cls_hs)\n","\n","        x = self.relu(x)\n","\n","        x = self.dropout(x)\n","\n","        # output layer\n","        x = self.fc2(x)\n","\n","        # apply softmax activation\n","        x = self.softmax(x)\n","        return x"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"1v_PNGIRBnh2","executionInfo":{"status":"ok","timestamp":1616624781462,"user_tz":-60,"elapsed":9795,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert,label_map)\n","\n","# push the model to GPU\n","model = model.to(device)"],"execution_count":282,"outputs":[]},{"cell_type":"code","metadata":{"id":"M4DlMLanBngB","executionInfo":{"status":"ok","timestamp":1616624781464,"user_tz":-60,"elapsed":9792,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = 1e-3)"],"execution_count":283,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T6XD6vwPBnd-","executionInfo":{"status":"ok","timestamp":1616624781465,"user_tz":-60,"elapsed":9786,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"0f72598d-fdb6-4e7b-8c14-bee5ab5469ef"},"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","#compute the class weights\n","class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(class_wts)"],"execution_count":284,"outputs":[{"output_type":"stream","text":["[0.61666667 0.77083333 0.77083333 0.71153846 5.28571429 5.28571429]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hCiLm-0jBnb6","executionInfo":{"status":"ok","timestamp":1616624781466,"user_tz":-60,"elapsed":9776,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["# convert class weights to tensor\n","weights= torch.tensor(class_wts,dtype=torch.float)\n","weights = weights.to(device)\n","\n","# loss function\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","# number of training epochs\n","epochs = 60"],"execution_count":285,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2ZKd7TMBnYI","executionInfo":{"status":"ok","timestamp":1616624781466,"user_tz":-60,"elapsed":9771,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["# function to train the model\n","def train():\n","    model.train()\n","\n","    total_loss, total_accuracy = 0, 0\n","  \n","    # empty list to save model predictions\n","    total_preds=[]\n","    total_labels =[]\n","  \n","    # iterate over batches\n","    for step,batch in enumerate(train_dataloader):\n","    \n","        # progress update after every 50 batches.\n","        if step % 100 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # clear previously calculated gradients \n","        model.zero_grad()        \n","\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds = preds.detach().cpu().numpy()\n","        preds = np.argmax(preds, axis=1)\n","        # append the model predictions\n","        total_preds+=list(preds)\n","        total_labels+=labels.tolist()\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","\n","    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    #total_preds  = np.concatenate(total_preds, axis=0)\n","    f1 = f1_score(total_labels, total_preds, average='weighted')\n","    #returns the loss and predictions\n","    return avg_loss, f1"],"execution_count":286,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2YqVCf2BnWC","executionInfo":{"status":"ok","timestamp":1616624781686,"user_tz":-60,"elapsed":9986,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["# function for evaluating the model\n","def evaluate():\n","  \n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","    total_labels = []\n","    # iterate over batches\n","    for step,batch in enumerate(val_dataloader):\n","    \n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","          # Calculate elapsed time in minutes.\n","          #elapsed = format_time(time.time() - t0)\n","\n","          # Report progress.\n","          print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds,labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","            preds = np.argmax(preds, axis=1)\n","            total_preds+=list(preds)\n","            total_labels+=labels.tolist()\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(val_dataloader) \n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    #total_preds  = np.concatenate(total_preds, axis=0)\n","    \n","    f1 = f1_score(total_labels, total_preds, average='weighted')\n","    return avg_loss, f1"],"execution_count":287,"outputs":[]},{"cell_type":"code","metadata":{"id":"fe0K6wptBnUF","executionInfo":{"status":"ok","timestamp":1616624781687,"user_tz":-60,"elapsed":9982,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["def save_checkpoint(filename, epoch, model, optimizer, label_map, id2label):\n","    state = {\n","        'epoch': epoch,\n","        'model': model,\n","        'optimizer': optimizer,\n","        'label_map': label_map,\n","        'id_map':id2label}\n","    torch.save(state, filename)"],"execution_count":288,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wl69CDIeBnSF","executionInfo":{"status":"ok","timestamp":1616625255668,"user_tz":-60,"elapsed":483958,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"a538e5ad-37dd-41a9-efb3-4e58c8523770"},"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","best_valid_f1 = float('-inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, f1_train = train()\n","    \n","    #evaluate model\n","    valid_loss, f1_valid = evaluate()\n","\n","    #save the best f1\n","    if f1_valid > best_valid_f1:\n","        print('\\nNEW BEST VALID F1 {:.3f} Valid Loss {:.3f}  Epoch  {:}'.format(best_valid_f1, valid_loss, epoch + 1))\n","        best_valid_f1 = f1_valid\n","        file_name = class_type+'_saved_weights_f1.pt'\n","        save_checkpoint(file_name, epoch, model, optimizer, label_map, id2label)\n","\n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        print('\\nNEW BEST VALID LOSS {:.3f} Valid_f1 {:.3f}  Epoch  {:}'.format(best_valid_loss, f1_valid, epoch + 1))\n","        best_valid_loss = valid_loss\n","        file_name = class_type+'_saved_weights_loss.pt'\n","        save_checkpoint(file_name, epoch, model, optimizer, label_map, id2label)\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n","    print(f'\\nTraining F1: {f1_train:.3f}')\n","    print(f'Validation F1: {f1_valid:.3f}')\n","\n","print('\\n BEST VALID LOSS FOUND {:.3f} BEST VALID F1 FOUND {:.3f}'.format(best_valid_loss, best_valid_f1))"],"execution_count":289,"outputs":[{"output_type":"stream","text":["\n"," Epoch 1 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 -inf Valid Loss 1.845  Epoch  1\n","\n","NEW BEST VALID LOSS inf Valid_f1 0.348  Epoch  1\n","\n","Training Loss: 1.809\n","Validation Loss: 1.845\n","\n","Training F1: 0.227\n","Validation F1: 0.348\n","\n"," Epoch 2 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.348 Valid Loss 1.556  Epoch  2\n","\n","NEW BEST VALID LOSS 1.845 Valid_f1 0.389  Epoch  2\n","\n","Training Loss: 1.714\n","Validation Loss: 1.556\n","\n","Training F1: 0.355\n","Validation F1: 0.389\n","\n"," Epoch 3 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.389 Valid Loss 1.571  Epoch  3\n","\n","Training Loss: 1.641\n","Validation Loss: 1.571\n","\n","Training F1: 0.433\n","Validation F1: 0.409\n","\n"," Epoch 4 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 1.556 Valid_f1 0.358  Epoch  4\n","\n","Training Loss: 1.567\n","Validation Loss: 1.457\n","\n","Training F1: 0.419\n","Validation F1: 0.358\n","\n"," Epoch 5 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.409 Valid Loss 1.399  Epoch  5\n","\n","NEW BEST VALID LOSS 1.457 Valid_f1 0.506  Epoch  5\n","\n","Training Loss: 1.497\n","Validation Loss: 1.399\n","\n","Training F1: 0.437\n","Validation F1: 0.506\n","\n"," Epoch 6 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 1.399 Valid_f1 0.433  Epoch  6\n","\n","Training Loss: 1.402\n","Validation Loss: 1.349\n","\n","Training F1: 0.479\n","Validation F1: 0.433\n","\n"," Epoch 7 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.506 Valid Loss 1.276  Epoch  7\n","\n","NEW BEST VALID LOSS 1.349 Valid_f1 0.541  Epoch  7\n","\n","Training Loss: 1.328\n","Validation Loss: 1.276\n","\n","Training F1: 0.487\n","Validation F1: 0.541\n","\n"," Epoch 8 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 1.276 Valid_f1 0.506  Epoch  8\n","\n","Training Loss: 1.327\n","Validation Loss: 1.206\n","\n","Training F1: 0.501\n","Validation F1: 0.506\n","\n"," Epoch 9 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.541 Valid Loss 1.087  Epoch  9\n","\n","NEW BEST VALID LOSS 1.206 Valid_f1 0.570  Epoch  9\n","\n","Training Loss: 1.265\n","Validation Loss: 1.087\n","\n","Training F1: 0.504\n","Validation F1: 0.570\n","\n"," Epoch 10 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.570 Valid Loss 1.120  Epoch  10\n","\n","Training Loss: 1.224\n","Validation Loss: 1.120\n","\n","Training F1: 0.556\n","Validation F1: 0.588\n","\n"," Epoch 11 / 60\n","\n","Evaluating...\n","\n","Training Loss: 1.169\n","Validation Loss: 1.174\n","\n","Training F1: 0.577\n","Validation F1: 0.523\n","\n"," Epoch 12 / 60\n","\n","Evaluating...\n","\n","Training Loss: 1.156\n","Validation Loss: 1.163\n","\n","Training F1: 0.541\n","Validation F1: 0.576\n","\n"," Epoch 13 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 1.087 Valid_f1 0.558  Epoch  13\n","\n","Training Loss: 1.086\n","Validation Loss: 1.003\n","\n","Training F1: 0.529\n","Validation F1: 0.558\n","\n"," Epoch 14 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.588 Valid Loss 0.954  Epoch  14\n","\n","NEW BEST VALID LOSS 1.003 Valid_f1 0.676  Epoch  14\n","\n","Training Loss: 1.113\n","Validation Loss: 0.954\n","\n","Training F1: 0.550\n","Validation F1: 0.676\n","\n"," Epoch 15 / 60\n","\n","Evaluating...\n","\n","Training Loss: 1.021\n","Validation Loss: 0.963\n","\n","Training F1: 0.628\n","Validation F1: 0.635\n","\n"," Epoch 16 / 60\n","\n","Evaluating...\n","\n","Training Loss: 1.026\n","Validation Loss: 0.980\n","\n","Training F1: 0.590\n","Validation F1: 0.616\n","\n"," Epoch 17 / 60\n","\n","Evaluating...\n","\n","Training Loss: 1.019\n","Validation Loss: 0.995\n","\n","Training F1: 0.589\n","Validation F1: 0.617\n","\n"," Epoch 18 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.938\n","Validation Loss: 0.995\n","\n","Training F1: 0.578\n","Validation F1: 0.568\n","\n"," Epoch 19 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.915\n","Validation Loss: 1.006\n","\n","Training F1: 0.638\n","Validation F1: 0.660\n","\n"," Epoch 20 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 0.954 Valid_f1 0.641  Epoch  20\n","\n","Training Loss: 0.925\n","Validation Loss: 0.899\n","\n","Training F1: 0.639\n","Validation F1: 0.641\n","\n"," Epoch 21 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.888\n","Validation Loss: 1.020\n","\n","Training F1: 0.632\n","Validation F1: 0.591\n","\n"," Epoch 22 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 0.899 Valid_f1 0.666  Epoch  22\n","\n","Training Loss: 0.898\n","Validation Loss: 0.893\n","\n","Training F1: 0.615\n","Validation F1: 0.666\n","\n"," Epoch 23 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 1.097\n","\n","Training F1: 0.608\n","Validation F1: 0.617\n","\n"," Epoch 24 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 0.893 Valid_f1 0.601  Epoch  24\n","\n","Training Loss: 0.861\n","Validation Loss: 0.843\n","\n","Training F1: 0.672\n","Validation F1: 0.601\n","\n"," Epoch 25 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.871\n","Validation Loss: 0.943\n","\n","Training F1: 0.615\n","Validation F1: 0.639\n","\n"," Epoch 26 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 0.843 Valid_f1 0.643  Epoch  26\n","\n","Training Loss: 0.858\n","Validation Loss: 0.840\n","\n","Training F1: 0.635\n","Validation F1: 0.643\n","\n"," Epoch 27 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 0.929\n","\n","Training F1: 0.642\n","Validation F1: 0.672\n","\n"," Epoch 28 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.676 Valid Loss 1.018  Epoch  28\n","\n","Training Loss: 0.828\n","Validation Loss: 1.018\n","\n","Training F1: 0.656\n","Validation F1: 0.677\n","\n"," Epoch 29 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.677 Valid Loss 0.881  Epoch  29\n","\n","Training Loss: 0.860\n","Validation Loss: 0.881\n","\n","Training F1: 0.648\n","Validation F1: 0.700\n","\n"," Epoch 30 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.725\n","Validation Loss: 0.925\n","\n","Training F1: 0.680\n","Validation F1: 0.646\n","\n"," Epoch 31 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 0.840 Valid_f1 0.697  Epoch  31\n","\n","Training Loss: 0.809\n","Validation Loss: 0.808\n","\n","Training F1: 0.671\n","Validation F1: 0.697\n","\n"," Epoch 32 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.893\n","Validation Loss: 0.962\n","\n","Training F1: 0.629\n","Validation F1: 0.624\n","\n"," Epoch 33 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.787\n","Validation Loss: 0.817\n","\n","Training F1: 0.661\n","Validation F1: 0.695\n","\n"," Epoch 34 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.716\n","Validation Loss: 0.839\n","\n","Training F1: 0.720\n","Validation F1: 0.673\n","\n"," Epoch 35 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.803\n","Validation Loss: 0.924\n","\n","Training F1: 0.655\n","Validation F1: 0.631\n","\n"," Epoch 36 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID LOSS 0.808 Valid_f1 0.697  Epoch  36\n","\n","Training Loss: 0.728\n","Validation Loss: 0.793\n","\n","Training F1: 0.695\n","Validation F1: 0.697\n","\n"," Epoch 37 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.699\n","Validation Loss: 1.050\n","\n","Training F1: 0.666\n","Validation F1: 0.563\n","\n"," Epoch 38 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.825\n","Validation Loss: 1.057\n","\n","Training F1: 0.663\n","Validation F1: 0.600\n","\n"," Epoch 39 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.672\n","Validation Loss: 1.049\n","\n","Training F1: 0.659\n","Validation F1: 0.608\n","\n"," Epoch 40 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.790\n","Validation Loss: 1.048\n","\n","Training F1: 0.639\n","Validation F1: 0.638\n","\n"," Epoch 41 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.700 Valid Loss 0.828  Epoch  41\n","\n","Training Loss: 0.745\n","Validation Loss: 0.828\n","\n","Training F1: 0.653\n","Validation F1: 0.719\n","\n"," Epoch 42 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.808\n","Validation Loss: 0.826\n","\n","Training F1: 0.659\n","Validation F1: 0.645\n","\n"," Epoch 43 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.660\n","Validation Loss: 0.863\n","\n","Training F1: 0.704\n","Validation F1: 0.692\n","\n"," Epoch 44 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.736\n","Validation Loss: 0.853\n","\n","Training F1: 0.695\n","Validation F1: 0.698\n","\n"," Epoch 45 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.714\n","Validation Loss: 1.023\n","\n","Training F1: 0.697\n","Validation F1: 0.701\n","\n"," Epoch 46 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.668\n","Validation Loss: 0.984\n","\n","Training F1: 0.706\n","Validation F1: 0.658\n","\n"," Epoch 47 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.673\n","Validation Loss: 0.845\n","\n","Training F1: 0.712\n","Validation F1: 0.709\n","\n"," Epoch 48 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.686\n","Validation Loss: 0.966\n","\n","Training F1: 0.697\n","Validation F1: 0.676\n","\n"," Epoch 49 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.653\n","Validation Loss: 0.877\n","\n","Training F1: 0.663\n","Validation F1: 0.675\n","\n"," Epoch 50 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.592\n","Validation Loss: 0.830\n","\n","Training F1: 0.696\n","Validation F1: 0.659\n","\n"," Epoch 51 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.621\n","Validation Loss: 0.973\n","\n","Training F1: 0.715\n","Validation F1: 0.654\n","\n"," Epoch 52 / 60\n","\n","Evaluating...\n","\n","NEW BEST VALID F1 0.719 Valid Loss 0.840  Epoch  52\n","\n","Training Loss: 0.590\n","Validation Loss: 0.840\n","\n","Training F1: 0.710\n","Validation F1: 0.732\n","\n"," Epoch 53 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.607\n","Validation Loss: 0.832\n","\n","Training F1: 0.744\n","Validation F1: 0.708\n","\n"," Epoch 54 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.664\n","Validation Loss: 0.826\n","\n","Training F1: 0.717\n","Validation F1: 0.711\n","\n"," Epoch 55 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.623\n","Validation Loss: 0.878\n","\n","Training F1: 0.726\n","Validation F1: 0.679\n","\n"," Epoch 56 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.645\n","Validation Loss: 0.889\n","\n","Training F1: 0.718\n","Validation F1: 0.639\n","\n"," Epoch 57 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.678\n","Validation Loss: 1.062\n","\n","Training F1: 0.701\n","Validation F1: 0.649\n","\n"," Epoch 58 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.577\n","Validation Loss: 1.109\n","\n","Training F1: 0.753\n","Validation F1: 0.649\n","\n"," Epoch 59 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.695\n","Validation Loss: 0.945\n","\n","Training F1: 0.703\n","Validation F1: 0.709\n","\n"," Epoch 60 / 60\n","\n","Evaluating...\n","\n","Training Loss: 0.708\n","Validation Loss: 0.942\n","\n","Training F1: 0.703\n","Validation F1: 0.673\n","\n"," BEST VALID LOSS FOUND0.793 BEST VALID F1 FOUND 0.732\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Ffhs2wKBnQH","executionInfo":{"status":"ok","timestamp":1616625263694,"user_tz":-60,"elapsed":491968,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"cc852966-84b6-44fb-b804-c21caf9278a1"},"source":["path = class_type+'_saved_weights_loss.pt' #!!!!!\n","test_df = ld.validation_data_frame\n","\n","checkpoint = torch.load(path, map_location=device)\n","model = checkpoint.get(\"model\")\n","\n","tokenizer = BertTokenizer.from_pretrained(berts[language])\n","\n","# weights_path = os.path.join(data_dir,\"Weights\",\"topic_saved_weights.pt\")\n","!cp learning_saved_weights.pt /content/drive/My\\ Drive/Tesi/Code/Intent_classification/Weights\n","!cp grounding_saved_weights.pt /content/drive/My\\ Drive/Tesi/Code/Intent_classification/Weights\n","\n","# tokenize and encode sequences in the test set\n","test_text,test_labels = test_df[\"query\"],test_df[\"category\"]\n","\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = max_seq_len,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# for test set\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n","print(\"test_y:\",test_y)"],"execution_count":290,"outputs":[{"output_type":"stream","text":["cp: cannot stat 'learning_saved_weights.pt': No such file or directory\n","cp: cannot stat 'grounding_saved_weights.pt': No such file or directory\n","test_y: tensor([0, 0, 3, 3, 0, 0, 2, 2, 0, 5, 2, 3, 1, 4, 1, 0, 3, 2, 0, 5, 2, 2, 1, 0,\n","        1, 1, 0, 2, 3, 2, 3, 2, 4, 0, 3, 2, 2, 1, 2, 1, 2, 0, 1, 2, 2, 1, 2, 3,\n","        2, 2, 3, 2, 2, 2, 2, 3, 4, 3, 0, 4, 2, 2, 3, 2, 3, 2, 2, 5, 5, 1, 2, 3,\n","        3, 1, 1, 2, 5, 0, 2, 5, 1, 1, 4, 1, 0, 0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OVVuj22DBnN4","executionInfo":{"status":"ok","timestamp":1616625263694,"user_tz":-60,"elapsed":491956,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["# get predictions for test data\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = preds.detach().cpu().numpy()"],"execution_count":291,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vOcGUj_qBnLr","executionInfo":{"status":"ok","timestamp":1616625263695,"user_tz":-60,"elapsed":491952,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"328ce934-835e-4c34-da32-6f0ff4fd9ce1"},"source":["preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"],"execution_count":292,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.75      0.80      0.77        15\n","           1       1.00      0.80      0.89        15\n","           2       1.00      0.93      0.97        30\n","           3       0.88      1.00      0.94        15\n","           4       1.00      1.00      1.00         5\n","           5       0.75      1.00      0.86         6\n","\n","    accuracy                           0.91        86\n","   macro avg       0.90      0.92      0.90        86\n","weighted avg       0.92      0.91      0.91        86\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ou9Wb49hB5Zj","executionInfo":{"status":"ok","timestamp":1616625604317,"user_tz":-60,"elapsed":424,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["max_seq_len=9 #!|!!\n","\n","class Prediction:\n","    def __init__(self):\n","        path = class_type+'_saved_weights_f1.pt'  #!!!!!\n","\n","        checkpoint = torch.load(path,map_location=device)\n","        self.predictor = checkpoint.get(\"model\")\n","        self.tokenizer = BertTokenizer.from_pretrained(berts[language])\n","        self.tag = checkpoint.get(\"id_map\")\n","\n","    def predict(self,text):\n","        tokens = self.tokenizer.tokenize(text)\n","        tokens = tokens[:max_seq_len - 2]\n","        tokens = ['[CLS]'] + tokens + ['[SEP]']\n","\n","        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","        input_ids = input_ids + [0] * (max_seq_len-len(input_ids))\n","        input_ids = torch.tensor(input_ids).unsqueeze(0)\n","        input_ids = input_ids.to(device)\n","\n","        input_mask = [1]*len(tokens) + [0] * (max_seq_len - len(tokens))\n","        input_mask = torch.tensor(input_mask).unsqueeze(0)\n","        input_mask = input_mask.to(device)\n","\n","        logits = self.predictor(input_ids,input_mask)\n","        prob = torch.nn.functional.softmax(logits,dim=1)\n","        result = [(self.tag[idx],item *100) for idx,item in enumerate(prob[0].tolist())]\n","        preds = logits.detach().cpu().numpy()\n","        pred_val = np.argmax(preds)\n","        pred_val = self.tag[pred_val]\n","        return result,pred_val\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrC10gQtB5W1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616625608082,"user_tz":-60,"elapsed":978,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"acb2967f-06cc-4bcf-a487-5a24c81ff71b"},"source":["pred = Prediction()\n","\n","print(max_seq_len)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dGuWxJXB5Tz","executionInfo":{"status":"ok","timestamp":1616625762296,"user_tz":-60,"elapsed":415,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}},"outputId":"03a19079-2e2e-46fb-f80a-dca281fe6eb7"},"source":["list_input = [\n","     'di che colore è',\n","     'che tessitura ha questo oggetto',\n","     'che cosa è questo',\n","     'che forma ha',\n","     'cosa è',\n","     'che superficie ha questo oggetto',\n","     'texture',\n","     'è un toroide',\n","     'abbandona',\n","     'ammazzati',\n","     'ti posso insegnare qualcosa',\n","     'Petrucci'     \n","     ] \n","\n","for item in list_input:\n","    confidence,pred_val = pred.predict(item)\n","    prob = round([i for i in confidence if i[0]==pred_val][0][1],4)\n","    print(\"'\" + item + \"' = \" + pred_val + \": \" + str(prob))\n","    print([(i[0],round(i[1],4)) for i in confidence])\n","    print()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["'di che colore è' = color_query: 98.1084\n","[('texture_query', 1.1794), ('shape_query', 0.6634), ('general_query', 0.0488), ('color_query', 98.1084), ('exit', 0.0), ('training_mode', 0.0001)]\n","\n","'che tessitura ha questo oggetto' = texture_query: 72.2735\n","[('texture_query', 72.2735), ('shape_query', 2.3775), ('general_query', 4.4984), ('color_query', 20.6624), ('exit', 0.0893), ('training_mode', 0.0989)]\n","\n","'che cosa è questo' = general_query: 98.9833\n","[('texture_query', 0.8596), ('shape_query', 0.0425), ('general_query', 98.9833), ('color_query', 0.0821), ('exit', 0.0043), ('training_mode', 0.0282)]\n","\n","'che forma ha' = shape_query: 99.2984\n","[('texture_query', 0.5004), ('shape_query', 99.2984), ('general_query', 0.059), ('color_query', 0.1421), ('exit', 0.0001), ('training_mode', 0.0)]\n","\n","'cosa è' = general_query: 98.9998\n","[('texture_query', 0.8754), ('shape_query', 0.0195), ('general_query', 98.9998), ('color_query', 0.069), ('exit', 0.0042), ('training_mode', 0.0321)]\n","\n","'che superficie ha questo oggetto' = texture_query: 54.359\n","[('texture_query', 54.359), ('shape_query', 6.3928), ('general_query', 13.3928), ('color_query', 25.8275), ('exit', 0.0088), ('training_mode', 0.0191)]\n","\n","'texture' = texture_query: 86.5415\n","[('texture_query', 86.5415), ('shape_query', 0.4022), ('general_query', 1.2394), ('color_query', 11.0518), ('exit', 0.7003), ('training_mode', 0.0648)]\n","\n","'è un toroide' = shape_query: 89.2286\n","[('texture_query', 4.9082), ('shape_query', 89.2286), ('general_query', 0.9469), ('color_query', 4.7853), ('exit', 0.0031), ('training_mode', 0.1278)]\n","\n","'abbandona' = exit: 97.9998\n","[('texture_query', 0.1105), ('shape_query', 0.0648), ('general_query', 0.115), ('color_query', 0.0221), ('exit', 97.9998), ('training_mode', 1.6878)]\n","\n","'ammazzati' = training_mode: 43.4718\n","[('texture_query', 26.0067), ('shape_query', 1.4738), ('general_query', 5.4718), ('color_query', 8.4095), ('exit', 15.1664), ('training_mode', 43.4718)]\n","\n","'ti posso insegnare qualcosa' = training_mode: 96.9972\n","[('texture_query', 0.9781), ('shape_query', 0.405), ('general_query', 1.5137), ('color_query', 0.0906), ('exit', 0.0153), ('training_mode', 96.9972)]\n","\n","'Petrucci' = color_query: 77.0587\n","[('texture_query', 8.7832), ('shape_query', 0.1615), ('general_query', 1.3281), ('color_query', 77.0587), ('exit', 0.0832), ('training_mode', 12.5852)]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ik5Ph-NFYnuw","executionInfo":{"status":"ok","timestamp":1616625483963,"user_tz":-60,"elapsed":6885,"user":{"displayName":"Davide Iraci","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEZfBIJCY52WxLY0E2IrD9frPLvSBomnnkEYUUvA=s64","userId":"18223997902854243280"}}},"source":["!cp grounding_saved_weights_loss.pt /content/drive/My\\ Drive/Tesi/Code/Intent_classification/Weights\n","!cp grounding_saved_weights_f1.pt /content/drive/My\\ Drive/Tesi/Code/Intent_classification/Weights"],"execution_count":5,"outputs":[]}]}